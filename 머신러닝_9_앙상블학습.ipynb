{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157623b2",
   "metadata": {},
   "source": [
    "# 앙상블 학습(Ensemble Learning)\n",
    "- 트레이닝 데이터를 기반으로 분류 모형을 여러 개 만들고 서로 비교하는 것이 핵심 아이디어\n",
    "- 여러 개의 분류기(classifier)을 결합함으로써 개별적인 분류기보다 성능이 뛰어난 최종 분류기를 만드는 것이 주 목적\n",
    "\n",
    "#####  Q: 앙상블 학습 모형이 개별 모형보다 성능이 뛰어난 이유?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95683cbb",
   "metadata": {},
   "source": [
    "## 보팅 (Voting)\n",
    "- 여러 개의 분류 모형의 결과를 대상으로 투표를 통해 최종 클래스 라벨을 정하는 방법\n",
    "- 다수결 투표(plurality voting), 과반수 투표(majority voting)\n",
    "- 개별 분류기는 여러 가지 알고리즘 사용해서 만들 수 있음: 로지스틱 회귀 ,svm, dt 등\n",
    "- 개별 분류기들이 동일한 트레이닝 데이터로 학습!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77951c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기 (꽃 데이터 분류)\n",
    "from sklearn import datasets\n",
    "raw_iris = datasets.load_iris()\n",
    "\n",
    "# 피처, 타깃 데이터 지정\n",
    "X = raw_iris.data\n",
    "y = raw_iris.target\n",
    "\n",
    "# 트레이닝/테스트 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# 데이터 표준화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(X_tn)\n",
    "X_tn_std = std_scale.transform(X_tn)\n",
    "X_te_std = std_scale.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a7e0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(multi_class='multinomial',\n",
       "                                                 random_state=1)),\n",
       "                             ('svm', SVC(kernel='linear', random_state=1)),\n",
       "                             ('gnb', GaussianNB())],\n",
       "                 weights=[1, 1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 학습\n",
    "# 보팅 학습 (여러 가지 머신러닝 모형 이용하는 방법)\n",
    "from sklearn.linear_model import LogisticRegression # 로지스틱 회귀 분석\n",
    "from sklearn import svm # 서포트 벡터 머신\n",
    "from sklearn.naive_bayes import GaussianNB # 가우시안 나이브 베이즈\n",
    "from sklearn.ensemble import VotingClassifier # 보팅을 위한 함수 (회귀 문제라면 VotingRegressor 사용)\n",
    "\n",
    "clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n",
    "clf2 = svm.SVC(kernel='linear', random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "# 세 가지 모형 이용해 보팅 모형 설정\n",
    "# estimators: 미리 만드느 세 가지 모형 의미\n",
    "# hard(기본값): 투표 결과로 과반수가 넘는 라벨이 정해지고/soft: 확률이 가장 높은 라벨로 정해짐\n",
    "# weights: 세 가지 모형의 비율\n",
    "clf_voting = VotingClassifier(estimators=[\n",
    "                                    ('lr', clf1),\n",
    "                                    ('svm', clf2),\n",
    "                                    ('gnb', clf3)\n",
    "],\n",
    "                              voting = 'hard',\n",
    "                              weights = [1, 1, 1])\n",
    "clf_voting.fit(X_tn_std, y_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856c5137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 예측\n",
    "pred_voting = clf_voting.predict(X_te_std)\n",
    "print(pred_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ee8144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# 정확도 평가\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_te, pred_voting)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cc8f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix 확인\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_te, pred_voting)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42dcfd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 분류 리포트 확인\n",
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_te, pred_voting)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1a86e",
   "metadata": {},
   "source": [
    "## 배깅과 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f13f62",
   "metadata": {},
   "source": [
    "### 독립적 앙상블 방법(Independent Ensemble Method)\n",
    "- 개별 분류기들: 서로 독립적인 알고리즘. 각 분류기는 서로 다른 머신러닝 알고리즘 사용 가능\n",
    "- 각 분류기가 독립적이므로 효과적으로 병렬화 가능\n",
    "#### 배깅(bootstrap aggregating)\n",
    "- 개별 분류기들의 분류 결과 종합하여 최종 분류기의 성능 향상\n",
    "- 오리지널 트레이닝 데이터 셋에서 부트스트랩(bootstrap) 샘플(중복을 허용한 랜덤 샘플)을 뽑아 학습\n",
    "- 개별 분류 모형의 결괏값을 모아 다수결 투표(plurality voting)를 통해 최종 예측하게 됨\n",
    "- 주로 배깅에 사용하는 개별 분류기들은 모두 같은 머신러닝 알고리즘\n",
    "#### 랜덤 포레스트(random forest)\n",
    "- 배깅 이용한 가장 유명한 알고리즘\n",
    "- 여러 개의 개별 분류기인 의사결정나무를 토대로 예측한 결과를 종합해 전체 예측 정확도 높이는 방법\n",
    "> <랜덤 포레스트 과정>\n",
    "> - n개의 데이터 랜덤 추출 (중복 가능)\n",
    "> - p개의 피처 선택 (중복 불가능)\n",
    "> - 의사 결정 나무 학습\n",
    "> - (1)-(3) 반복\n",
    "> - 각 의사 결정 나무별 결과를 투표를 통해 클래스 레이블 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd45757b",
   "metadata": {},
   "source": [
    "##### 랜덤 포레스트 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943b30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "raw_wine = datasets.load_wine()\n",
    "\n",
    "X = raw_wine.data\n",
    "y = raw_wine.target\n",
    "\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y, random_state=0)\n",
    "\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(X_tn)\n",
    "X_tn_std = std_scale.transform(X_tn)\n",
    "X_te_std = std_scale.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367ab22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 학습\n",
    "from sklearn.ensemble import RandomForestClassifier # 회귀 문제면 RandomForestRegressor 사용\n",
    "clf_rf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf_rf.fit(X_tn_std, y_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577e01da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 0 1 1 0 2 1 1 2 2 0 1 2 1 0 0 2 0 0 0 0 1 1 1 1 1 1 2 0 0 1 0 0 0 2\n",
      " 1 1 2 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 예측\n",
    "pred_rf = clf_rf.predict(X_te_std)\n",
    "print(pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dec323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# 정확도 평가\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_te, pred_rf)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ecec248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 1 19  1]\n",
      " [ 0  0  8]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix 확인\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_te, pred_rf)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7bf697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        16\n",
      "           1       1.00      0.90      0.95        21\n",
      "           2       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.94      0.97      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 분류 리포트 확인\n",
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_te, pred_rf)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f708af6",
   "metadata": {},
   "source": [
    "##### 배깅 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23102819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 0 1 1 0 2 1 1 2 2 0 1 2 1 0 0 2 0 0 0 0 1 1 1 1 1 1 2 0 0 1 0 0 0 2\n",
      " 1 1 2 0 0 1 1 1]\n",
      "0.9555555555555556\n",
      "[[16  0  0]\n",
      " [ 1 19  1]\n",
      " [ 0  0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        16\n",
      "           1       1.00      0.90      0.95        21\n",
      "           2       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.94      0.97      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB # 개별 분류기로 사용할 함수\n",
    "from sklearn.ensemble import BaggingClassifier # 배깅을 위한 함수. 회귀 문제라면 BaggingRegressor 사용\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "raw_wine = datasets.load_wine()\n",
    "\n",
    "X = raw_wine.data\n",
    "y = raw_wine.target\n",
    "\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y, random_state=0)\n",
    "\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(X_tn)\n",
    "X_tn_std = std_scale.transform(X_tn)\n",
    "X_te_std = std_scale.transform(X_te)\n",
    "\n",
    "# best_estimator에 개별 학습기, n_estimators에 개별 분류기 개수\n",
    "clf_bagging = BaggingClassifier(base_estimator=GaussianNB(), n_estimators=10, random_state=0)\n",
    "clf_bagging.fit(X_tn_std, y_tn)\n",
    "\n",
    "pred_clf = clf_bagging.predict(X_te_std)\n",
    "print(pred_clf)\n",
    "\n",
    "accuracy = accuracy_score(y_te, pred_clf)\n",
    "print(accuracy)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_te, pred_clf)\n",
    "print(conf_matrix)\n",
    "\n",
    "class_report = classification_report(y_te, pred_clf)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342470f4",
   "metadata": {},
   "source": [
    "## 부스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24577c8",
   "metadata": {},
   "source": [
    "### 의존적 앙상블 방법(Dependent Ensemble Method)\n",
    "- 개별 학습기들이 서로 독립이 아닌 경우\n",
    "\n",
    "#### 부스팅(Boosting)\n",
    "- 의존적 앙상블 방법 중 가장 유명한 방법\n",
    "- 핵심 아이디어: 분류하기 어려운 데이터에 집중\n",
    "- 약한 학습기(weak learner) 여러 개를 모아 하나의 강한 학습기 만드는 방법\n",
    "- 관심의 정도 반영. 점차 학습 진행되면서 올바르게 분류된 데이터 포인트의 가중치는 감소, 잘못 분류된 데이터 포인트의 가중치는 증가 (학습이 진행되면서 분류하기 어려운 데이터에 집중)\n",
    "- 이전 단계에서 만들어진 학습기는 다음 단계에서 사용할 트레이닝 셋의 가중치를 변경하는데 사용\n",
    "- 배깅과 달리 이전 분류기의 성능에 영향 받음 (새로운 분류기는 이전 분류기의 성능에 따라 잘못 분류된 데이터에 더 집중)\n",
    "- 각 데이터 포인트에 할당된 가중치에 비례해서 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9545e9",
   "metadata": {},
   "source": [
    "##### 에이다 부스트(AdaBoost)\n",
    "- 분류하기 어려운 트레이닝 데이터에 가중치를 더 높이는 것 (잘못 분류된 트레이닝 데이터 포인트 가중치 증가)\n",
    "- 약한 분류기의 결과를 모아 투표 실시하지만, 투표할 때 약한 분류기별로 투표 결과에 가중치 부여\n",
    "- 일반 부스팅과는 다르게 약한 학습기 훈련할 때 훈련 데이터 셋 전체 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd1bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유방암 여부 예측\n",
    "from sklearn import datasets\n",
    "raw_breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "X = raw_breast_cancer.data\n",
    "y = raw_breast_cancer.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(X_tn)\n",
    "X_tn_std = std_scale.transform(X_tn)\n",
    "X_te_std = std_scale.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23da00b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier # AdaBoostRegressor\n",
    "clf_ada = AdaBoostClassifier(random_state=0)\n",
    "clf_ada.fit(X_tn_std, y_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2328b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0\n",
      " 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "pred_ada = clf_ada.predict(X_te_std)\n",
    "print(pred_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016a3e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_te, pred_ada)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07919817",
   "metadata": {},
   "source": [
    "##### 그래디언트 부스팅 (Gradient Boosting)\n",
    "- 비용함수를 최적화시킴으로써 학습 능력 향상하는 알고리즘\n",
    "\n",
    "\n",
    "- \"확률적 그래디언트 부스팅(Stochatic Gradient Boosting)\">> 사이킷런>GradientBoostingRegressor 실행할 때 subsample 사용하면 각 트리가 학습할 때 사용하느 트레이닝 데이터의 비율 정할 수 있음 (ex. subsample=0.2: 각 트리는 트레이닝 데이터의 20% 비율로 데이터를 랜덤으로 선택)\n",
    " \n",
    " \n",
    "- XGBoost\n",
    "- LightGBM: XGBoost가 무겁다는 단점 보완"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de5191eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유방암 여부 예측\n",
    "from sklearn import datasets\n",
    "raw_breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "X = raw_breast_cancer.data\n",
    "y = raw_breast_cancer.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(X_tn)\n",
    "X_tn_std = std_scale.transform(X_tn)\n",
    "X_te_std = std_scale.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca9ada15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.01, max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 학습\n",
    "from sklearn.ensemble import GradientBoostingClassifier # GradientBoostingRegressor\n",
    "clf_gbt = GradientBoostingClassifier(max_depth=2, learning_rate=0.01, random_state=0)\n",
    "clf_gbt.fit(X_tn_std, y_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6847052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1\n",
      " 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 예측\n",
    "pred_gboost = clf_gbt.predict(X_te_std)\n",
    "print(pred_gboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe1ea5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# 정확도 평가\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_te, pred_gboost)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187f8a9",
   "metadata": {},
   "source": [
    "## 스태킹(Stacking)\n",
    "- 여러 가지 학습기를 쌓는 방법\n",
    "- 베이스 학습기(base learner)가 먼저 학습한 후 메타 학습기(meta learner)는 베이스 학습기의 예측을 피처 데이터로 활용해 최종 예측\n",
    "> <스태킹 알고리즘>\n",
    "> - (전체데이터 = 트레이닝 셋 + 테스트셋) 중 트레이닝 데이터를 두 개로 분리\n",
    "> - 베이스 학습기로 첫 번째 트레이닝 데이터 셋 학습\n",
    "> - 학습된 베이스 학습기에 두 번째 트레이닝 데이터 셋 넣고 예측\n",
    "> - 예측값을 또 다른 인풋 데이터로 활용해 메타 학습기를 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c888d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유방암 여부 예측\n",
    "from sklearn import datasets\n",
    "raw_breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "X = raw_breast_cancer.data\n",
    "y = raw_breast_cancer.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(X_tn)\n",
    "X_tn_std = std_scale.transform(X_tn)\n",
    "X_te_std = std_scale.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d83583db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('svm', SVC(kernel='linear', random_state=1)),\n",
       "                               ('gnb', GaussianNB())],\n",
       "                   final_estimator=LogisticRegression())"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 학습\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "clf1 = svm.SVC(kernel='linear', random_state=1)\n",
    "clf2 = GaussianNB()\n",
    "\n",
    "# 기본 학습기로 svm, 가우시안 나이브 베이즈 모형 사용\n",
    "# 메타 학습기로 로지스틱 회귀 분석 사용\n",
    "# StackingClassifier 이용해서 스태킹 모형 설정 (회귀 문제는 StackingRegressor)\n",
    "clf_stkg = StackingClassifier(\n",
    "            estimators=[('svm', clf1),\n",
    "                       ('gnb', clf2)],\n",
    "            final_estimator=LogisticRegression())\n",
    "clf_stkg.fit(X_tn_std, y_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75992ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0\n",
      " 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 예측\n",
    "pred_stkg = clf_stkg.predict(X_te_std)\n",
    "print(pred_stkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a686066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# 정확도 평가\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_te, pred_stkg)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3b3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
